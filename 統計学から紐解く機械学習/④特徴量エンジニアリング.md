# 【python】統計学から紐解く機械学習-④特徴量エンジニアリング-

本シリーズでは、統計学の視点から機械学習を紐解くことを主眼に、基礎的な理論とPythonを用いた実装例を交えて、実践的に解説していきます。
アルゴリズムの仕組みや数式の背景だけでなく「なぜその処理が必要なのか」、「どのように判断すべきか」といった思考プロセスにも焦点を当てます。

## 目次
- はじめに
- 特徴量エンジニアリングとは
- 統計学的視点から見た特徴量エンジニアリング
- 特徴量エンジニアリングの主要手法
  - ℹ️特徴量生成（Feature Generation）
  - ℹ️特徴量変換（Feature Transformation）
  - ℹ️特徴量選択（Feature Selection）
  - ℹ️次元削減（Dimensionality Reduction）
- Pythonを用いた実践
- まとめ

## はじめに

本シリーズでは全6ステップに分けて、統計学と機械学習の関係性について解説しています。

1. 概要
2. 探索的データ分析(EDA)
3. データ前処理
4. **特徴量エンジニアリング**
5. モデル選定と学習
6. モデル評価とチューニング

第3回「データ前処理編」では、生データを機械学習に適した形に変換する手法について解説しました。
今回は、前処理されたデータから、より予測精度の高いモデルを構築するための新たな特徴量を創出・選択する手法について詳しく解説します。

## 特徴量エンジニアリングとは

特徴量エンジニアリング（Feature Engineering）とは、ドメイン知識と統計的手法を組み合わせて、機械学習モデルの予測性能を向上させるための特徴量を設計・選択・変換するプロセスです。

「データとアルゴリズムがあれば自動的に良いモデルができる」という誤解がありますが、実際には適切な特徴量設計がモデルの性能を大きく左右します。Andrew Ng氏の言葉を借りれば、「Coming up with features is difficult, time-consuming, requires expert knowledge」であり、機械学習プロジェクトの成功における最も重要な要素の一つです。

### 特徴量エンジニアリングの目的

1. **予測精度の向上**：より情報量の多い特徴量の創出
2. **計算効率の改善**：不要な特徴量の除去による次元削減
3. **モデルの解釈性向上**：ビジネス的に意味のある特徴量の構築
4. **汎化性能の向上**：過学習を防ぐ適切な特徴量選択

## 統計学的視点から見た特徴量エンジニアリング

統計学では、変数間の関係性や分布の特性を分析することで、データの潜在的な構造を理解します。
特徴量エンジニアリングにおいても、統計学的知識を活用することで、より効果的な特徴量を設計できます。

### 統計学的根拠に基づく特徴量設計

**相関分析に基づく特徴量生成**
- 高い相関を持つ変数の組み合わせ
- 非線形関係の捕捉（交互作用項の追加）

**分布特性に基づく変換**
- 歪んだ分布の正規化（対数変換、Box-Cox変換）
- 外れ値に頑健な特徴量（ランク変換、分位点変換）

**情報理論に基づく特徴量選択**
- 相互情報量による特徴量の重要度評価
- エントロピーに基づく冗長性の除去

## 特徴量エンジニアリングの主要手法

### ℹ️特徴量生成（Feature Generation）

既存の特徴量から新しい特徴量を作成する手法です。ドメイン知識と統計的洞察を組み合わせることが重要です。

#### 数学的演算による生成

**四則演算**
- 比率特徴量：売上高利益率 = 利益 ÷ 売上高
- 差分特徴量：利益率の変化 = 今期利益率 - 前期利益率
- 積特徴量：総合スコア = 品質スコア × 価格スコア

**統計量の計算**
- 移動平均：時系列データの平滑化
- 標準偏差：変動の大きさを表現
- パーセンタイル：相対的な位置を表現

#### 交互作用項の追加

2つ以上の特徴量の組み合わせ効果を捕捉します。
```
交互作用項 = 特徴量A × 特徴量B
```

統計学的解釈：一方の特徴量の効果が、もう一方の特徴量の値によって変化する関係性を表現

#### 多項式特徴量

非線形関係を線形モデルで表現するための手法です。
```
2次項: x²
3次項: x³
交差項: x₁ × x₂
```

### ℹ️特徴量変換（Feature Transformation）

既存の特徴量の分布や尺度を変更して、モデルの学習効率を向上させる手法です。

#### 数学的変換

**対数変換**
- 右に歪んだ分布を正規分布に近づける
- 比例関係を加法関係に変換
- 適用条件：正の値のみ、指数的な増加パターン

```python
log_feature = np.log(feature + 1)  # +1は0値への対応
```

**Box-Cox変換**
- 正規分布への変換を自動的に最適化
- λパラメータを推定して最適な変換を決定

```python
from scipy.stats import boxcox
transformed_data, lambda_param = boxcox(data)
```

**平方根変換**
- 分散を安定化させる効果
- カウントデータに適用されることが多い

#### 分位点変換

分布の形状に関係なく一様分布や正規分布に変換します。
- 外れ値に頑健
- 非線形な変換による情報の保持

### ℹ️特徴量選択（Feature Selection）

多数の特徴量から予測に有効な特徴量のみを選択する手法です。統計的検定や情報理論を活用します。

#### フィルタ法（Filter Methods）

統計的指標に基づいて特徴量を選択します。

**単変量統計検定**
- 回帰問題：F検定、相関係数
- 分類問題：カイ二乗検定、相互情報量

**相関に基づく選択**
- 目的変数との相関が高い特徴量を選択
- 特徴量間の相関が高い場合は一方を除去（多重共線性対策）

#### ラッパー法（Wrapper Methods）

機械学習モデルの性能を評価指標として特徴量を選択します。

**再帰的特徴量除去（RFE）**
1. すべての特徴量でモデルを学習
2. 重要度の低い特徴量を除去
3. 指定した特徴量数になるまで繰り返し

**前進選択・後退除去**
- 前進選択：重要な特徴量を段階的に追加
- 後退除去：すべての特徴量から不要なものを段階的に除去

#### 埋め込み法（Embedded Methods）

モデルの学習過程で特徴量選択を同時に行います。

**LASSO回帰**
- L1正則化により自動的に特徴量選択
- 回帰係数を0に収束させることで特徴量を除去

**ランダムフォレストの特徴量重要度**
- Gini不純度やエントロピーの減少量で重要度を計算
- 複数の決定木の平均で安定した重要度を取得

### ℹ️次元削減（Dimensionality Reduction）

高次元データを低次元空間に射影して計算効率と汎化性能を向上させる手法です。

#### 主成分分析（PCA）

分散を最大化する方向（主成分）を見つけて次元削減を行います。

**統計学的原理**
- 共分散行列の固有ベクトルが主成分方向
- 固有値が各主成分の分散（情報量）

**適用条件**
- 線形関係を仮定
- 特徴量間の相関が存在する場合に効果的

#### t-SNE（t-distributed Stochastic Neighbor Embedding）

非線形次元削減手法で、局所的な構造を保持します。

**特徴**
- 高次元での近傍関係を低次元で再現
- 可視化に優れているが、新しいデータへの適用が困難

#### 線形判別分析（LDA）

クラス間分離を最大化する方向に射影する教師ありの次元削減手法です。

**統計学的原理**
- クラス内分散を最小化
- クラス間分散を最大化

## Pythonを用いた実践

diamondsデータセットを用いて、実際の特徴量エンジニアリングを実装します。

### ライブラリのインポートとデータ準備

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import boxcox
import warnings
warnings.filterwarnings('ignore')

# データの読み込みと前処理
df = sns.load_dataset('diamonds')

# カテゴリ変数のエンコーディング
cut_order = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}
color_order = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}
clarity_order = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8, 'FL': 9}

df['cut_encoded'] = df['cut'].map(cut_order)
df['color_encoded'] = df['color'].map(color_order)
df['clarity_encoded'] = df['clarity'].map(clarity_order)

print("データ形状:", df.shape)
print("\n基本情報:")
print(df[['carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table', 'x', 'y', 'z', 'price']].describe())
```

### 特徴量生成（Feature Generation）

```python
# 1. 数学的演算による特徴量生成
df_engineered = df.copy()

# 体積の計算（x, y, zの積）
df_engineered['volume'] = df_engineered['x'] * df_engineered['y'] * df_engineered['z']

# 表面積の概算
df_engineered['surface_area'] = 2 * (df_engineered['x'] * df_engineered['y'] + 
                                    df_engineered['y'] * df_engineered['z'] + 
                                    df_engineered['z'] * df_engineered['x'])

# 密度（カラット/体積）
df_engineered['density'] = df_engineered['carat'] / (df_engineered['volume'] + 1e-8)  # ゼロ除算回避

# アスペクト比
df_engineered['aspect_ratio_xy'] = df_engineered['x'] / (df_engineered['y'] + 1e-8)
df_engineered['aspect_ratio_xz'] = df_engineered['x'] / (df_engineered['z'] + 1e-8)

# 品質スコア（重み付き合計）
df_engineered['quality_score'] = (0.4 * df_engineered['cut_encoded'] + 
                                 0.3 * df_engineered['color_encoded'] + 
                                 0.3 * df_engineered['clarity_encoded'])

# サイズカテゴリ（カラットによる分類）
df_engineered['size_category'] = pd.cut(df_engineered['carat'], 
                                       bins=[0, 0.5, 1.0, 2.0, np.inf], 
                                       labels=['small', 'medium', 'large', 'extra_large'])
df_engineered['size_category_encoded'] = df_engineered['size_category'].astype('category').cat.codes

print("生成した特徴量:")
new_features = ['volume', 'surface_area', 'density', 'aspect_ratio_xy', 'aspect_ratio_xz', 'quality_score', 'size_category_encoded']
print(df_engineered[new_features].describe())

# 相関の確認
correlation_with_price = df_engineered[new_features + ['price']].corr()['price'].sort_values(ascending=False)
print("\n価格との相関:")
print(correlation_with_price)
```

### 特徴量変換（Feature Transformation）

```python
# 2. 数学的変換
df_transformed = df_engineered.copy()

# 対数変換（右に歪んだ分布を正規化）
skewed_features = ['carat', 'price', 'x', 'y', 'z', 'volume']

for feature in skewed_features:
    if feature in df_transformed.columns:
        # 対数変換（+1は0値への対応）
        df_transformed[f'{feature}_log'] = np.log1p(df_transformed[feature])

# Box-Cox変換（priceのみ適用）
price_positive = df_transformed['price'][df_transformed['price'] > 0]
if len(price_positive) > 0:
    price_boxcox, lambda_param = boxcox(price_positive)
    # 元のデータフレームに結果を格納（サイズを合わせるため）
    df_transformed['price_boxcox'] = np.nan
    df_transformed.loc[df_transformed['price'] > 0, 'price_boxcox'] = price_boxcox
    print(f"Box-Cox変換のλパラメータ: {lambda_param:.4f}")

# 平方根変換
for feature in ['carat', 'volume']:
    if feature in df_transformed.columns:
        df_transformed[f'{feature}_sqrt'] = np.sqrt(df_transformed[feature])

# 変換効果の可視化
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# 元のprice分布
df_transformed['price'].hist(bins=50, ax=axes[0, 0])
axes[0, 0].set_title('Original Price Distribution')

# 対数変換後
df_transformed['price_log'].hist(bins=50, ax=axes[0, 1])
axes[0, 1].set_title('Log-transformed Price Distribution')

# Box-Cox変換後
df_transformed['price_boxcox'].hist(bins=50, ax=axes[0, 2])
axes[0, 2].set_title('Box-Cox transformed Price Distribution')

# 元のcarat分布
df_transformed['carat'].hist(bins=50, ax=axes[1, 0])
axes[1, 0].set_title('Original Carat Distribution')

# 対数変換後
df_transformed['carat_log'].hist(bins=50, ax=axes[1, 1])
axes[1, 1].set_title('Log-transformed Carat Distribution')

# 平方根変換後
df_transformed['carat_sqrt'].hist(bins=50, ax=axes[1, 2])
axes[1, 2].set_title('Square-root transformed Carat Distribution')

plt.tight_layout()
plt.show()

print("変換後の統計量:")
transform_features = ['price_log', 'price_boxcox', 'carat_log', 'carat_sqrt']
print(df_transformed[transform_features].describe())
```

### 多項式特徴量と交互作用項

```python
# 3. 多項式特徴量の生成
# 主要な数値特徴量を選択
key_features = ['carat', 'depth', 'table', 'cut_encoded', 'color_encoded', 'clarity_encoded']
X_key = df_transformed[key_features].fillna(0)

# 2次多項式特徴量の生成
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
X_poly = poly.fit_transform(X_key)

# 特徴量名の取得
poly_feature_names = poly.get_feature_names_out(key_features)

print(f"元の特徴量数: {X_key.shape[1]}")
print(f"多項式特徴量数: {X_poly.shape[1]}")
print("\n生成された特徴量の例:")
for i, name in enumerate(poly_feature_names[:15]):  # 最初の15個を表示
    print(f"{i+1:2d}. {name}")

# 交互作用項のみを抽出
interaction_mask = []
for i, name in enumerate(poly_feature_names):
    # 交互作用項の判定（異なる変数の積）
    if ' ' in name and '^' not in name and name.count(' ') == 1:
        interaction_mask.append(i)

X_interactions = X_poly[:, interaction_mask]
interaction_names = [poly_feature_names[i] for i in interaction_mask]

print(f"\n交互作用項の数: {len(interaction_names)}")
print("主要な交互作用項:")
for name in interaction_names:
    print(f"- {name}")
```

### 特徴量選択（Feature Selection）

```python
# 4. 統計的特徴量選択
# データの準備
feature_columns = ['carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table', 'x', 'y', 'z'] + new_features
X = df_engineered[feature_columns].fillna(0)
y = df_engineered['price']

# 訓練・テストセットの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# フィルタ法：F検定による特徴量選択
selector_f = SelectKBest(score_func=f_regression, k=10)
X_train_selected_f = selector_f.fit_transform(X_train, y_train)
X_test_selected_f = selector_f.transform(X_test)

# 選択された特徴量の表示
selected_features_f = [feature_columns[i] for i in selector_f.get_support(indices=True)]
feature_scores_f = selector_f.scores_[selector_f.get_support()]

print("F検定による特徴量選択結果:")
for feature, score in zip(selected_features_f, feature_scores_f):
    print(f"{feature}: {score:.2f}")

# ラッパー法：RFE（再帰的特徴量除去）
estimator = LinearRegression()
rfe = RFE(estimator=estimator, n_features_to_select=10)
X_train_selected_rfe = rfe.fit_transform(X_train, y_train)
X_test_selected_rfe = rfe.transform(X_test)

selected_features_rfe = [feature_columns[i] for i in rfe.get_support(indices=True)]
print("\nRFEによる特徴量選択結果:")
for i, feature in enumerate(selected_features_rfe):
    ranking = rfe.ranking_[feature_columns.index(feature)]
    print(f"{feature}: ranking {ranking}")

# 埋め込み法：LASSO回帰
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X_train, y_train)

# LASSO係数の表示
lasso_coefficients = pd.DataFrame({
    'feature': feature_columns,
    'coefficient': lasso.coef_
})
lasso_selected = lasso_coefficients[lasso_coefficients['coefficient'] != 0]
lasso_selected = lasso_selected.reindex(lasso_selected['coefficient'].abs().sort_values(ascending=False).index)

print("\nLASSO回帰による特徴量選択結果:")
print(lasso_selected)
```

### ランダムフォレストによる特徴量重要度

```python
# 5. ランダムフォレストによる特徴量重要度
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 特徴量重要度の取得
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': rf.feature_importances_
})
feature_importance = feature_importance.sort_values('importance', ascending=False)

print("ランダムフォレスト特徴量重要度:")
print(feature_importance.head(10))

# 特徴量重要度の可視化
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Feature Importance')
plt.title('Top 15 Feature Importance (Random Forest)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 相関マトリックスによる多重共線性チェック
correlation_matrix = X_train[selected_features_f].corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Correlation Matrix of Selected Features')
plt.tight_layout()
plt.show()

# 高い相関を持つ特徴量ペアの特定
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.8:  # 閾値0.8以上
            high_corr_pairs.append({
                'feature1': correlation_matrix.columns[i],
                'feature2': correlation_matrix.columns[j],
                'correlation': corr_value
            })

if high_corr_pairs:
    print("\n高い相関を持つ特徴量ペア（|r| > 0.8）:")
    for pair in high_corr_pairs:
        print(f"{pair['feature1']} - {pair['feature2']}: {pair['correlation']:.3f}")
else:
    print("\n高い相関を持つ特徴量ペアは見つかりませんでした。")
```

### 主成分分析（PCA）による次元削減

```python
# 6. 主成分分析
# データの標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# PCAの実行
pca = PCA()
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# 累積寄与率の計算
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

# 寄与率の可視化
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Individual Explained Variance Ratio')

plt.subplot(1, 2, 2)
plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Cumulative Explained Variance Ratio')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# 95%の分散を説明する成分数
n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
print(f"95%の分散を説明するのに必要な主成分数: {n_components_95}")
print(f"元の特徴量数: {X_train.shape[1]}")
print(f"次元削減率: {(1 - n_components_95/X_train.shape[1])*100:.1f}%")

# 最適な成分数でPCAを再実行
pca_optimal = PCA(n_components=n_components_95)
X_train_pca_optimal = pca_optimal.fit_transform(X_train_scaled)
X_test_pca_optimal = pca_optimal.transform(X_test_scaled)

print(f"\n削減後の特徴量形状: {X_train_pca_optimal.shape}")
```

### 特徴量エンジニアリング効果の評価

```python
# 7. 特徴量エンジニアリング効果の評価
from sklearn.metrics import mean_squared_error, r2_score

def evaluate_features(X_train, X_test, y_train, y_test, feature_name):
    """特徴量セットを評価する関数"""
    # 線形回帰モデルで評価
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    return {
        'feature_set': feature_name,
        'mse': mse,
        'rmse': np.sqrt(mse),
        'r2_score': r2,
        'n_features': X_train.shape[1]
    }

# 各特徴量セットの評価
results = []

# 1. 元の特徴量のみ
original_features = ['carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table', 'x', 'y', 'z']
X_train_original = X_train[original_features]
X_test_original = X_test[original_features]
results.append(evaluate_features(X_train_original, X_test_original, y_train, y_test, 'Original Features'))

# 2. エンジニアリングした特徴量を追加
X_train_engineered = X_train
X_test_engineered = X_test
results.append(evaluate_features(X_train_engineered, X_test_engineered, y_train, y_test, 'Engineered Features'))

# 3. F検定で選択した特徴量
results.append(evaluate_features(X_train_selected_f, X_test_selected_f, y_train, y_test, 'F-test Selected'))

# 4. RFEで選択した特徴量
results.append(evaluate_features(X_train_selected_rfe, X_test_selected_rfe, y_train, y_test, 'RFE Selected'))

# 5. PCAで次元削減した特徴量
results.append(evaluate_features(X_train_pca_optimal, X_test_pca_optimal, y_train, y_test, 'PCA Reduced'))

# 結果の表示
results_df = pd.DataFrame(results)
print("特徴量エンジニアリング効果の比較:")
print(results_df.round(4))

# 結果の可視化
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# R²スコア
axes[0].bar(results_df['feature_set'], results_df['r2_score'])
axes[0].set_title('R² Score Comparison')
axes[0].set_ylabel('R² Score')
axes[0].tick_params(axis='x', rotation=45)

# RMSE
axes[1].bar(results_df['feature_set'], results_df['rmse'])
axes[1].set_title('RMSE Comparison')
axes[1].set_ylabel('RMSE')
axes[1].tick_params(axis='x', rotation=45)

# 特徴量数
axes[2].bar(results_df['feature_set'], results_df['n_features'])
axes[2].set_title('Number of Features')
axes[2].set_ylabel('Number of Features')
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 最良の結果を表示
best_result = results_df.loc[results_df['r2_score'].idxmax()]
print(f"\n最高性能の特徴量セット: {best_result['feature_set']}")
print(f"R²スコア: {best_result['r2_score']:.4f}")
print(f"RMSE: {best_result['rmse']:.2f}")
print(f"特徴量数: {int(best_result['n_features'])}")
```

## まとめ

特徴量エンジニアリングは機械学習の性能を大きく左右する重要なプロセスです。統計学的知識を活用することで：

1. **客観的な特徴量評価**：統計的検定や情報理論に基づく特徴量の重要度評価
2. **効果的な特徴量変換**：データの分布特性に応じた適切な変換手法の選択
3. **多重共線性対策**：相関分析による冗長な特徴量の特定と除去
4. **次元削減の活用**：計算効率と汎化性能のバランスを考慮した次元削減

これらの手法を適切に組み合わせることで、モデルの予測精度向上と計算効率の改善を同時に実現できます。

次回は、「モデル選定と学習」をテーマに、特徴量エンジニアリングされたデータを用いて最適な機械学習アルゴリズムを選択し、効果的に学習させる手法について詳しく解説します。
ぜひ引き続きご覧ください。