# 【python】統計学から紐解く機械学習-⑥モデル評価とチューニング-

本シリーズでは、統計学の視点から機械学習を紐解くことを主眼に、基礎的な理論とPythonを用いた実装例を交えて、実践的に解説していきます。
アルゴリズムの仕組みや数式の背景だけでなく「なぜその処理が必要なのか」、「どのように判断すべきか」といった思考プロセスにも焦点を当てます。

## 目次
- はじめに
- モデル評価とチューニングとは
- 統計学的視点から見たモデル評価
- 評価指標の理論的背景
  - ℹ️回帰問題の評価指標
  - ℹ️分類問題の評価指標
- ハイパーパラメータチューニング
  - ℹ️グリッドサーチ
  - ℹ️ランダムサーチ
  - ℹ️ベイズ最適化
- 交差検証の高度な手法
- モデルの解釈性と説明可能性
- Pythonを用いた実践
- まとめ

## はじめに

本シリーズでは全6ステップに分けて、統計学と機械学習の関係性について解説しています。

1. 概要
2. 探索的データ分析(EDA)
3. データ前処理
4. 特徴量エンジニアリング
5. モデル選定と学習
6. **モデル評価とチューニング**

第5回「モデル選定と学習編」では、適切なアルゴリズムの選択と学習プロセスについて解説しました。
今回は最終回として、学習したモデルの性能を客観的に評価し、ハイパーパラメータの最適化によってモデル性能を最大化する手法について詳しく解説します。

## モデル評価とチューニングとは

モデル評価（Model Evaluation）とは、学習したモデルの予測性能を定量的に測定し、実用性を判断するプロセスです。
モデルチューニング（Model Tuning）とは、ハイパーパラメータの調整によってモデルの性能を最適化するプロセスです。

### なぜ適切な評価とチューニングが重要なのか

1. **汎化性能の正確な把握**：未見データに対する真の性能を推定
2. **過学習の検出**：訓練データに過度に適合していないかの確認
3. **ビジネス価値の定量化**：実際の業務における価値の測定
4. **モデル改善の方向性**：性能向上のための具体的な施策の特定

## 統計学的視点から見たモデル評価

### 統計的推論の観点

機械学習における評価は、本質的に統計的推論の問題です。
限られたサンプルから母集団の性質を推定し、信頼区間を構築することが重要です。

**推定の不偏性**
```
E[θ̂] = θ (真の値)
```

**推定の一致性**
```
lim P(|θ̂n - θ| > ε) = 0
n→∞
```

### 評価における統計的仮定

**独立同分布（i.i.d.）仮定**
- 各サンプルが独立
- 同一の分布から生成

**代表性の仮定**
- 訓練データとテストデータが同一分布
- 本番環境のデータとの分布一致

## 評価指標の理論的背景

### ℹ️回帰問題の評価指標

#### 平均二乗誤差（MSE）
```
MSE = (1/n) Σ(yi - ŷi)²
```

**統計学的意味**
- 不偏推定量（バイアスがない場合）
- 外れ値に敏感
- 正規分布を仮定した最尤推定に対応

#### 平均絶対誤差（MAE）
```
MAE = (1/n) Σ|yi - ŷi|
```

**統計学的意味**
- 中央値の最適推定量
- 外れ値に頑健
- ラプラス分布を仮定した最尤推定に対応

#### 決定係数（R²）
```
R² = 1 - (SS_res / SS_tot)
```
```
SS_res = Σ(yi - ŷi)²
SS_tot = Σ(yi - ȳ)²
```

**統計学的意味**
- 説明できる分散の割合
- 0から1の範囲（負値も可能）
- 相関係数の二乗として解釈可能

#### 平均絶対パーセント誤差（MAPE）
```
MAPE = (100/n) Σ|((yi - ŷi) / yi)|
```

**適用条件**
- 目的変数が正の値のみ
- パーセンテージ解釈が重要な場合

### ℹ️分類問題の評価指標

#### 混同行列（Confusion Matrix）

|         | 予測Positive | 予測Negative |
|---------|-------------|-------------|
| 実際Positive | TP | FN |
| 実際Negative | FP | TN |

#### 精度（Accuracy）
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

#### 適合率（Precision）
```
Precision = TP / (TP + FP)
```

#### 再現率（Recall）
```
Recall = TP / (TP + FN)
```

#### F1スコア
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**統計学的解釈**
- 精度と再現率の調和平均
- 不均衡データセットに適用

#### ROC曲線とAUC

**ROC曲線**：真陽性率（TPR）と偽陽性率（FPR）の関係
```
TPR = TP / (TP + FN)
FPR = FP / (FP + TN)
```

**AUC（Area Under Curve）**
- 0.5から1.0の範囲
- 0.5：ランダム予測
- 1.0：完全予測

**統計学的意味**
- ランダムに選んだ正例と負例のペアで、正例により高いスコアが付く確率

## ハイパーパラメータチューニング

### ℹ️グリッドサーチ

全ての組み合わせを網羅的に探索する手法です。

**利点**
- 確実に最適解を発見
- 各パラメータの影響を把握可能

**欠点**
- 計算コストが指数的に増加
- 次元の呪い

### ℹ️ランダムサーチ

パラメータ空間からランダムにサンプリングする手法です。

**統計学的根拠**
- 高次元において効率的
- 重要でないパラメータの影響を削減

**適用条件**
- パラメータ数が多い場合
- 計算資源が限られている場合

### ℹ️ベイズ最適化

ガウス過程を用いて効率的に最適化を行う手法です。

**統計学的背景**
- ベイズ統計に基づく不確実性の定量化
- 獲得関数による効率的な探索

**獲得関数の例**
- Expected Improvement（EI）
- Upper Confidence Bound（UCB）
- Probability of Improvement（PI）

## 交差検証の高度な手法

### 層化交差検証（Stratified CV）

クラスの比率を保持した交差検証です。

**適用条件**
- 不均衡データセット
- 分類問題

### 時系列交差検証（Time Series CV）

時系列データに適した交差検証です。

**特徴**
- 未来の情報を使用しない
- 時間的な順序を保持

### 群交差検証（Group CV）

関連するサンプルをグループとして扱う交差検証です。

**適用例**
- 患者データ（同一患者の複数測定）
- 地理的データ（同一地域の複数観測）

## モデルの解釈性と説明可能性

### SHAP（SHapley Additive exPlanations）

ゲーム理論のシャープレイ値に基づく特徴量重要度の計算手法です。

**統計学的背景**
- 各特徴量の限界的貢献度を計算
- 加法性を満たす唯一の手法

### LIME（Local Interpretable Model-agnostic Explanations）

局所的な線形近似による説明手法です。

**特徴**
- モデルに依存しない
- 個別予測の説明

### 順列重要度（Permutation Importance）

特徴量をランダムに入れ替えた際の性能劣化を測定する手法です。

**統計学的解釈**
- 因果関係ではなく関連性を測定
- モデルに依存しない手法

## Pythonを用いた実践

diamondsデータセットとアイリスデータセットを用いて、実際のモデル評価とチューニングを実装します。

### 回帰問題の詳細評価

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV, 
                                   RandomizedSearchCV, TimeSeriesSplit, StratifiedKFold)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,
                           mean_absolute_percentage_error, make_scorer)
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# データ準備
df = sns.load_dataset('diamonds')

# カテゴリ変数のエンコーディング
cut_order = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}
color_order = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}
clarity_order = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8, 'FL': 9}

df['cut_encoded'] = df['cut'].map(cut_order)
df['color_encoded'] = df['color'].map(color_order)
df['clarity_encoded'] = df['clarity'].map(clarity_order)

# 特徴量エンジニアリング
df['volume'] = df['x'] * df['y'] * df['z']
df['density'] = df['carat'] / (df['volume'] + 1e-8)

features = ['carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table', 'volume', 'density']
X = df[features]
y = df['price']

# サンプリング（計算時間短縮のため）
X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, random_state=42)

# 訓練・テストセット分割
X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)

# 標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"データ形状: 訓練={X_train.shape}, テスト={X_test.shape}")
```

### 複数評価指標による包括的評価

```python
# カスタム評価関数の定義
def calculate_regression_metrics(y_true, y_pred):
    """回帰問題の包括的評価指標を計算"""
    metrics = {
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred),
        'R²': r2_score(y_true, y_pred),
        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100
    }
    
    # 調整済み決定係数
    n = len(y_true)
    p = X_train.shape[1]  # 特徴量数
    metrics['Adjusted_R²'] = 1 - (1 - metrics['R²']) * (n - 1) / (n - p - 1)
    
    return metrics

# ベースラインモデルの評価
models = {
    'Ridge': Ridge(alpha=1.0, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

baseline_results = {}

for name, model in models.items():
    if name == 'Ridge':
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    baseline_results[name] = calculate_regression_metrics(y_test, y_pred)

# 結果の表示
baseline_df = pd.DataFrame(baseline_results).T
print("ベースラインモデルの評価結果:")
print(baseline_df.round(4))

# 結果の可視化
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.ravel()

metrics = ['MSE', 'RMSE', 'MAE', 'R²', 'Adjusted_R²', 'MAPE']

for i, metric in enumerate(metrics):
    values = baseline_df[metric]
    axes[i].bar(values.index, values)
    axes[i].set_title(f'{metric} Comparison')
    axes[i].set_ylabel(metric)
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

### 統計的有意性の検定

```python
# 交差検証による統計的評価
def cross_validate_model(model, X, y, cv=5, scoring='r2'):
    """クロスバリデーションと統計的検定"""
    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
    
    # 統計的要約
    stats_summary = {
        'mean': scores.mean(),
        'std': scores.std(),
        'min': scores.min(),
        'max': scores.max(),
        'median': np.median(scores),
        'confidence_interval_95': stats.t.interval(0.95, len(scores)-1, 
                                                 loc=scores.mean(), 
                                                 scale=stats.sem(scores))
    }
    
    return scores, stats_summary

# 各モデルのクロスバリデーション
cv_results = {}

for name, model in models.items():
    print(f"\n{name}のクロスバリデーション結果:")
    
    if name == 'Ridge':
        scores, summary = cross_validate_model(model, X_train_scaled, y_train)
    else:
        scores, summary = cross_validate_model(model, X_train, y_train)
    
    cv_results[name] = {'scores': scores, 'summary': summary}
    
    print(f"R²スコア: {summary['mean']:.4f} ± {summary['std']:.4f}")
    print(f"95%信頼区間: [{summary['confidence_interval_95'][0]:.4f}, {summary['confidence_interval_95'][1]:.4f}]")

# 統計的有意差検定（対応のあるt検定）
from scipy.stats import ttest_rel

model_names = list(cv_results.keys())
print("\nモデル間の統計的有意差検定（対応のあるt検定）:")

for i in range(len(model_names)):
    for j in range(i+1, len(model_names)):
        model1, model2 = model_names[i], model_names[j]
        scores1 = cv_results[model1]['scores']
        scores2 = cv_results[model2]['scores']
        
        t_stat, p_value = ttest_rel(scores1, scores2)
        
        print(f"{model1} vs {model2}:")
        print(f"  t統計量: {t_stat:.4f}, p値: {p_value:.4f}")
        if p_value < 0.05:
            print(f"  → 有意差あり (p < 0.05)")
        else:
            print(f"  → 有意差なし (p ≥ 0.05)")
```

### グリッドサーチによるハイパーパラメータ最適化

```python
# Random Forestのハイパーパラメータ最適化
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# グリッドサーチの実行
rf_grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    rf_param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1,
    return_train_score=True
)

print("Random Forestのグリッドサーチ実行中...")
rf_grid_search.fit(X_train, y_train)

print(f"最適パラメータ: {rf_grid_search.best_params_}")
print(f"最良CV スコア: {rf_grid_search.best_score_:.4f}")

# グリッドサーチ結果の詳細分析
rf_results = pd.DataFrame(rf_grid_search.cv_results_)

# パラメータごとの性能分析
plt.figure(figsize=(15, 10))

# n_estimatorsの影響
plt.subplot(2, 3, 1)
n_est_scores = rf_results.groupby('param_n_estimators')['mean_test_score'].mean()
plt.bar(range(len(n_est_scores)), n_est_scores.values)
plt.xticks(range(len(n_est_scores)), n_est_scores.index)
plt.xlabel('n_estimators')
plt.ylabel('Mean CV Score')
plt.title('Impact of n_estimators')

# max_depthの影響
plt.subplot(2, 3, 2)
depth_scores = rf_results.groupby('param_max_depth')['mean_test_score'].mean()
plt.bar(range(len(depth_scores)), depth_scores.values)
plt.xticks(range(len(depth_scores)), [str(x) for x in depth_scores.index])
plt.xlabel('max_depth')
plt.ylabel('Mean CV Score')
plt.title('Impact of max_depth')

# min_samples_splitの影響
plt.subplot(2, 3, 3)
split_scores = rf_results.groupby('param_min_samples_split')['mean_test_score'].mean()
plt.bar(range(len(split_scores)), split_scores.values)
plt.xticks(range(len(split_scores)), split_scores.index)
plt.xlabel('min_samples_split')
plt.ylabel('Mean CV Score')
plt.title('Impact of min_samples_split')

# 学習曲線vs検定曲線
plt.subplot(2, 3, 4)
train_scores = rf_results['mean_train_score']
test_scores = rf_results['mean_test_score']
plt.scatter(train_scores, test_scores, alpha=0.6)
plt.plot([train_scores.min(), train_scores.max()], 
         [train_scores.min(), train_scores.max()], 'r--', linewidth=2)
plt.xlabel('Training Score')
plt.ylabel('Validation Score')
plt.title('Training vs Validation Score')

# バイアス-バリアンス分析
plt.subplot(2, 3, 5)
plt.scatter(rf_results['mean_train_score'] - rf_results['mean_test_score'], 
           rf_results['mean_test_score'], alpha=0.6)
plt.xlabel('Training - Validation Score')
plt.ylabel('Validation Score')
plt.title('Bias-Variance Analysis')
plt.axvline(x=0, color='r', linestyle='--')

plt.tight_layout()
plt.show()

# 最適モデルでの最終評価
best_rf = rf_grid_search.best_estimator_
y_pred_best_rf = best_rf.predict(X_test)
final_metrics = calculate_regression_metrics(y_test, y_pred_best_rf)

print("\n最適化後Random Forestの最終評価:")
for metric, value in final_metrics.items():
    print(f"{metric}: {value:.4f}")
```

### ランダムサーチとベイズ最適化

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# ランダムサーチの実行
rf_random_param = {
    'n_estimators': randint(50, 300),
    'max_depth': [None] + list(randint(5, 50).rvs(10)),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

rf_random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    rf_random_param,
    n_iter=50,  # 50回の試行
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42,
    return_train_score=True
)

print("Random Forestのランダムサーチ実行中...")
rf_random_search.fit(X_train, y_train)

print(f"ランダムサーチ最適パラメータ: {rf_random_search.best_params_}")
print(f"ランダムサーチ最良CVスコア: {rf_random_search.best_score_:.4f}")

# グリッドサーチvsランダムサーチの比較
comparison_results = {
    'Grid Search': {
        'best_score': rf_grid_search.best_score_,
        'n_trials': len(rf_grid_search.cv_results_['mean_test_score']),
        'best_params': rf_grid_search.best_params_
    },
    'Random Search': {
        'best_score': rf_random_search.best_score_,
        'n_trials': len(rf_random_search.cv_results_['mean_test_score']),
        'best_params': rf_random_search.best_params_
    }
}

print("\nグリッドサーチ vs ランダムサーチ比較:")
for method, results in comparison_results.items():
    print(f"\n{method}:")
    print(f"  最良スコア: {results['best_score']:.4f}")
    print(f"  試行回数: {results['n_trials']}")
    print(f"  最適パラメータ: {results['best_params']}")

# 効率性の可視化
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
grid_scores = rf_grid_search.cv_results_['mean_test_score']
plt.plot(range(len(grid_scores)), sorted(grid_scores, reverse=True), 'b-', label='Grid Search')
plt.xlabel('Number of Trials')
plt.ylabel('Best Score So Far')
plt.title('Grid Search Progress')
plt.grid(True)

plt.subplot(1, 2, 2)
random_scores = rf_random_search.cv_results_['mean_test_score']
cumulative_best = np.maximum.accumulate(sorted(random_scores, reverse=True))
plt.plot(range(len(cumulative_best)), cumulative_best, 'r-', label='Random Search')
plt.xlabel('Number of Trials')
plt.ylabel('Best Score So Far')
plt.title('Random Search Progress')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### 学習曲線と検証曲線

```python
from sklearn.model_selection import learning_curve, validation_curve

# 学習曲線の描画
def plot_learning_curves(model, X, y, title="Learning Curves"):
    """学習曲線をプロット"""
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='r2', random_state=42
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                     alpha=0.1, color='blue')
    
    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, 
                     alpha=0.1, color='red')
    
    plt.xlabel('Training Set Size')
    plt.ylabel('R² Score')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return train_sizes, train_scores, val_scores

# 検証曲線の描画
def plot_validation_curve(model, X, y, param_name, param_range, title="Validation Curve"):
    """検証曲線をプロット"""
    train_scores, val_scores = validation_curve(
        model, X, y, param_name, param_range, cv=5, 
        scoring='r2', n_jobs=-1
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(param_range, train_mean, 'o-', color='blue', label='Training Score')
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, 
                     alpha=0.1, color='blue')
    
    plt.plot(param_range, val_mean, 'o-', color='red', label='Validation Score')
    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, 
                     alpha=0.1, color='red')
    
    plt.xlabel(param_name)
    plt.ylabel('R² Score')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    if param_name in ['alpha', 'C']:
        plt.xscale('log')
    plt.show()

# Random Forestの学習曲線
print("Random Forestの学習曲線を描画中...")
plot_learning_curves(
    RandomForestRegressor(n_estimators=100, random_state=42),
    X_train, y_train,
    "Random Forest Learning Curves"
)

# n_estimatorsの検証曲線
print("n_estimatorsの検証曲線を描画中...")
plot_validation_curve(
    RandomForestRegressor(random_state=42),
    X_train, y_train,
    'n_estimators',
    [10, 25, 50, 75, 100, 150, 200, 250, 300],
    "Validation Curve: n_estimators"
)

# max_depthの検証曲線
print("max_depthの検証曲線を描画中...")
plot_validation_curve(
    RandomForestRegressor(n_estimators=100, random_state=42),
    X_train, y_train,
    'max_depth',
    [3, 5, 7, 10, 15, 20, 25, 30, None],
    "Validation Curve: max_depth"
)
```

### モデルの解釈性分析

```python
# SHAP値による特徴量重要度分析
try:
    import shap
    
    # 最適化されたRandom Forestモデルを使用
    explainer = shap.TreeExplainer(best_rf)
    shap_values = explainer.shap_values(X_test[:100])  # サンプル数を制限
    
    # SHAP Summary Plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, X_test[:100], feature_names=features, show=False)
    plt.title('SHAP Summary Plot')
    plt.tight_layout()
    plt.show()
    
    # 特徴量重要度の比較
    shap_importance = np.abs(shap_values).mean(0)
    rf_importance = best_rf.feature_importances_
    
    importance_comparison = pd.DataFrame({
        'feature': features,
        'SHAP_importance': shap_importance,
        'RF_importance': rf_importance
    }).sort_values('SHAP_importance', ascending=False)
    
    print("特徴量重要度の比較:")
    print(importance_comparison.round(4))
    
    # 重要度の可視化
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.barh(importance_comparison['feature'], importance_comparison['SHAP_importance'])
    plt.xlabel('SHAP Importance')
    plt.title('SHAP Feature Importance')
    plt.gca().invert_yaxis()
    
    plt.subplot(1, 2, 2)
    plt.barh(importance_comparison['feature'], importance_comparison['RF_importance'])
    plt.xlabel('Random Forest Importance')
    plt.title('Random Forest Feature Importance')
    plt.gca().invert_yaxis()
    
    plt.tight_layout()
    plt.show()
    
except ImportError:
    print("SHAPライブラリがインストールされていません。")
    print("pip install shap でインストールしてください。")

# 順列重要度の計算
from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(
    best_rf, X_test, y_test, n_repeats=10, random_state=42, scoring='r2'
)

perm_importance_df = pd.DataFrame({
    'feature': features,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

print("\n順列重要度:")
print(perm_importance_df.round(4))

# 順列重要度の可視化
plt.figure(figsize=(10, 6))
plt.barh(perm_importance_df['feature'], perm_importance_df['importance_mean'],
         xerr=perm_importance_df['importance_std'])
plt.xlabel('Permutation Importance')
plt.title('Permutation Feature Importance')
plt.gca().invert_yaxis()
plt.grid(True, axis='x')
plt.tight_layout()
plt.show()
```

### 分類問題での評価（ボーナス）

```python
# アイリスデータセットでの分類問題評価
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, roc_curve, precision_recall_curve,
                           average_precision_score)
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier

# データの読み込み
iris = load_iris()
X_iris, y_iris = iris.data, iris.target

# 訓練・テスト分割
X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(
    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris
)

# Random Forest分類器
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train_iris, y_train_iris)

# 予測と予測確率
y_pred_iris = rf_clf.predict(X_test_iris)
y_prob_iris = rf_clf.predict_proba(X_test_iris)

# 分類レポート
print("分類レポート:")
print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))

# 混同行列
cm = confusion_matrix(y_test_iris, y_pred_iris)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# ROC曲線（多クラス）
y_test_bin = label_binarize(y_test_iris, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

plt.figure(figsize=(12, 4))

for i in range(n_classes):
    plt.subplot(1, 3, i+1)
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob_iris[:, i])
    auc_score = roc_auc_score(y_test_bin[:, i], y_prob_iris[:, i])
    
    plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve: {iris.target_names[i]}')
    plt.legend(loc="lower right")
    plt.grid(True)

plt.tight_layout()
plt.show()

print(f"マクロ平均AUC: {roc_auc_score(y_test_bin, y_prob_iris, multi_class='ovr', average='macro'):.4f}")
print(f"重み付き平均AUC: {roc_auc_score(y_test_bin, y_prob_iris, multi_class='ovr', average='weighted'):.4f}")
```

### 最終的なモデル性能レポート

```python
# 最終レポートの生成
def generate_model_report(model, X_train, X_test, y_train, y_test, model_name):
    """包括的なモデル評価レポートを生成"""
    
    # 予測
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # 訓練・テストセットでの評価
    train_metrics = calculate_regression_metrics(y_train, y_pred_train)
    test_metrics = calculate_regression_metrics(y_test, y_pred_test)
    
    # クロスバリデーション
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    report = {
        'Model': model_name,
        'Training_R²': train_metrics['R²'],
        'Test_R²': test_metrics['R²'],
        'CV_R²_mean': cv_scores.mean(),
        'CV_R²_std': cv_scores.std(),
        'Training_RMSE': train_metrics['RMSE'],
        'Test_RMSE': test_metrics['RMSE'],
        'Overfitting_Gap': train_metrics['R²'] - test_metrics['R²'],
        'Generalization_Score': test_metrics['R²'] / train_metrics['R²'] if train_metrics['R²'] > 0 else 0
    }
    
    return report

# 最適化後の全モデルで最終評価
final_models = {
    'Optimized_RandomForest': best_rf,
    'Ridge_Baseline': Ridge(alpha=1.0, random_state=42),
    'GradientBoosting_Baseline': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

final_reports = []

for name, model in final_models.items():
    if 'Ridge' in name:
        # Ridgeは標準化されたデータを使用
        model.fit(X_train_scaled, y_train)
        report = generate_model_report(model, X_train_scaled, X_test_scaled, y_train, y_test, name)
    else:
        model.fit(X_train, y_train)
        report = generate_model_report(model, X_train, X_test, y_train, y_test, name)
    
    final_reports.append(report)

# 最終レポートの表示
final_report_df = pd.DataFrame(final_reports)
final_report_df = final_report_df.sort_values('Test_R²', ascending=False)

print("最終モデル評価レポート:")
print("=" * 80)
for _, row in final_report_df.iterrows():
    print(f"\nモデル: {row['Model']}")
    print(f"テストR²: {row['Test_R²']:.4f}")
    print(f"クロスバリデーションR²: {row['CV_R²_mean']:.4f} ± {row['CV_R²_std']:.4f}")
    print(f"テストRMSE: {row['Test_RMSE']:.2f}")
    print(f"過学習ギャップ: {row['Overfitting_Gap']:.4f}")
    print(f"汎化スコア: {row['Generalization_Score']:.4f}")

# 最良モデルの選定
best_final_model = final_report_df.iloc[0]
print(f"\n🏆 最良モデル: {best_final_model['Model']}")
print(f"   テスト性能: R² = {best_final_model['Test_R²']:.4f}, RMSE = {best_final_model['Test_RMSE']:.2f}")
print(f"   汎化能力: {best_final_model['Generalization_Score']:.4f}")

# 最終可視化
plt.figure(figsize=(15, 10))

# 性能比較
plt.subplot(2, 3, 1)
plt.bar(final_report_df['Model'], final_report_df['Test_R²'])
plt.ylabel('Test R²')
plt.title('Model Performance Comparison')
plt.xticks(rotation=45)

# 過学習分析
plt.subplot(2, 3, 2)
plt.scatter(final_report_df['Training_R²'], final_report_df['Test_R²'])
plt.plot([0, 1], [0, 1], 'r--', linewidth=2)
plt.xlabel('Training R²')
plt.ylabel('Test R²')
plt.title('Training vs Test Performance')

# RMSE比較
plt.subplot(2, 3, 3)
training_rmse = [calculate_regression_metrics(y_train, model.predict(X_train if 'Ridge' not in name else X_train_scaled))['RMSE'] 
                for name, model in final_models.items()]
plt.scatter(training_rmse, final_report_df['Test_RMSE'])
plt.xlabel('Training RMSE')
plt.ylabel('Test RMSE')
plt.title('Training vs Test RMSE')

# 汎化能力
plt.subplot(2, 3, 4)
plt.bar(final_report_df['Model'], final_report_df['Generalization_Score'])
plt.ylabel('Generalization Score')
plt.title('Generalization Ability')
plt.xticks(rotation=45)

# クロスバリデーション安定性
plt.subplot(2, 3, 5)
plt.bar(final_report_df['Model'], final_report_df['CV_R²_std'])
plt.ylabel('CV R² Standard Deviation')
plt.title('Cross-validation Stability')
plt.xticks(rotation=45)

# 総合スコア（Test R² - Overfitting Gap）
plt.subplot(2, 3, 6)
comprehensive_score = final_report_df['Test_R²'] - np.abs(final_report_df['Overfitting_Gap'])
plt.bar(final_report_df['Model'], comprehensive_score)
plt.ylabel('Comprehensive Score')
plt.title('Comprehensive Performance\n(Test R² - |Overfitting Gap|)')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()
```

## まとめ

モデル評価とチューニングは機械学習プロジェクトの成功を決定づける重要なステップです。本記事では統計学的視点から：

### 評価の重要ポイント

1. **複数評価指標の活用**：R²、RMSE、MAEなど問題に応じた適切な指標選択
2. **統計的有意性の検証**：信頼区間や仮説検定による客観的な性能評価
3. **クロスバリデーション**：過学習を防ぎ汎化性能を正確に推定
4. **学習曲線・検証曲線**：モデルの振る舞いの可視化と理解

### チューニングの効果的手法

1. **グリッドサーチ**：小規模パラメータ空間での網羅的探索
2. **ランダムサーチ**：高次元パラメータ空間での効率的探索
3. **ベイズ最適化**：少ない試行回数での効率的最適化
4. **アーリーストッピング**：過学習の防止

### モデルの解釈性

1. **SHAP値**：個別予測の説明と特徴量の貢献度
2. **順列重要度**：モデルに依存しない特徴量重要度
3. **学習曲線**：データ量と性能の関係性

### 統計学的アプローチの価値

統計学的知識を活用することで、機械学習プロジェクトにおいて：
- **客観的な判断基準**の確立
- **過学習リスクの定量化**と対策
- **モデル性能の信頼性評価**
- **ビジネス価値の定量化**

が可能になります。

本シリーズを通じて、統計学と機械学習の密接な関係性と、理論に基づいた実践的なアプローチの重要性をご理解いただけたでしょうか。
データサイエンスプロジェクトの成功には、アルゴリズムの知識だけでなく、統計学的思考が不可欠です。

ぜひ今後のプロジェクトでこれらの知識を活用し、より価値の高い機械学習システムの構築にお役立てください。

---

**シリーズ完結**

本シリーズ「統計学から紐解く機械学習」をお読みいただき、ありがとうございました。
機械学習の理論的背景から実践的な実装まで、統計学の視点から包括的に解説いたしました。
今後も皆様のデータサイエンス学習にお役立ていただければ幸いです。