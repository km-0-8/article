# 【python】統計学から紐解く機械学習-③データ前処理-

本シリーズでは、統計学の視点から機械学習を紐解くことを主眼に、基礎的な理論とPythonを用いた実装例を交えて、実践的に解説していきます。
アルゴリズムの仕組みや数式の背景だけでなく「なぜその処理が必要なのか」、「どのように判断すべきか」といった思考プロセスにも焦点を当てます。

## 目次
- はじめに
- データ前処理とは
- 統計学的視点から見たデータ前処理の重要性
- データ前処理の主要な手法
  - ℹ️欠損値処理
  - ℹ️外れ値処理
  - ℹ️データ型変換
  - ℹ️スケーリング（標準化・正規化）
  - ℹ️カテゴリ変数のエンコーディング
- Pythonを用いた実践
- まとめ

## はじめに

本シリーズでは全6ステップに分けて、統計学と機械学習の関係性について解説しています。

1. 概要
2. 探索的データ分析(EDA)
3. **データ前処理**
4. 特徴量エンジニアリング
5. モデル選定と学習
6. モデル評価とチューニング

第2回「探索的データ分析(EDA)編」では、データの特性を把握する方法について解説しました。
今回は、EDAで得られた洞察を元に、機械学習モデルに適した形にデータを変換するプロセスについて詳しく解説します。

## データ前処理とは

データ前処理（Data Preprocessing）とは、生データを機械学習アルゴリズムが効率的に学習できる形式に変換するプロセスです。
実務では「データサイエンスの作業時間の約80%を占める」と言われるほど重要かつ時間のかかる工程です。

データ前処理の目的は以下の通りです：
- **データ品質の向上**：欠損値や外れ値の適切な処理
- **アルゴリズムの制約への対応**：データ型やスケールの統一
- **学習効率の改善**：計算量の削減と収束性の向上
- **予測精度の向上**：ノイズの除去と信号の強化

## 統計学的視点から見たデータ前処理の重要性

統計学では、データの分布や特性を理解することで適切な分析手法を選択します。
機械学習においても同様に、データの統計的特性を理解することで、最適な前処理手法を選択できます。

### 統計学的根拠に基づく前処理の判断基準

**分布の形状による判断**
- 正規分布に近い → 標準化が効果的
- 歪んだ分布 → 対数変換やBox-Cox変換を検討
- 外れ値の存在 → ロバストスケーラーの使用

**相関関係による判断**
- 高い相関を持つ変数 → 多重共線性の回避
- 目的変数との相関 → 特徴量の重要度評価

## データ前処理の主要な手法

### ℹ️欠損値処理

欠損値（Missing Values）は、データ収集過程で生じる避けられない問題です。
適切な処理を行わないと、モデルの学習や予測に大きな影響を与えます。

#### 欠損値の種類と統計学的意味

**MCAR（Missing Completely At Random）**
- 欠損が完全にランダムに発生
- 例：機器の故障による測定失敗

**MAR（Missing At Random）**
- 観測された変数に依存して欠損が発生
- 例：高齢者の収入データが欠損しやすい

**MNAR（Missing Not At Random）**
- 欠損値自体に意味がある
- 例：プライバシーを理由とした回答拒否

#### 主な処理手法

**除去（Deletion）**
- リストワイズ削除：欠損値を含む行を削除
- ペアワイズ削除：分析に必要な変数のみで削除
- 適用条件：欠損率が低く、MCARの場合

**補完（Imputation）**
- 平均値補完：量的変数の中心傾向で補完
- 中央値補完：外れ値に頑健な補完
- 最頻値補完：質的変数に適用
- 回帰補完：他の変数から予測して補完
- 多重補完（Multiple Imputation）：不確実性を考慮

### ℹ️外れ値処理

外れ値（Outliers）は、他のデータと大きく異なる値を示すデータポイントです。
統計学的手法を用いて客観的に検出・処理することが重要です。

#### 外れ値検出手法

**統計的手法**
- Zスコア法：標準偏差を基準とした検出
  - |Z| > 3 で外れ値と判定
- IQR法：四分位範囲を基準とした検出
  - Q1 - 1.5×IQR または Q3 + 1.5×IQR を超える値

**機械学習手法**
- Isolation Forest：異常検知アルゴリズム
- Local Outlier Factor（LOF）：密度ベースの検出

#### 処理方法

**除去**
- 明らかな異常値や入力エラーの場合
- データ量が十分にある場合

**変換**
- 対数変換：右に歪んだ分布の正規化
- Winsorizing：極値を特定の百分位点に置換
- キャッピング：上下限値を設定

**そのまま保持**
- 業務上意味のある値の場合
- ロバストなアルゴリズムを使用する場合

### ℹ️データ型変換

機械学習アルゴリズムは数値データを前提とするため、文字列や日付データを適切な数値形式に変換する必要があります。

#### 日付・時刻データの処理

**時系列特徴量への分解**
- 年、月、日、曜日、時間への分解
- 季節性や周期性の特徴量化
- 祝日フラグの作成

**相対的時間の計算**
- 基準日からの経過日数
- イベントからの経過時間

### ℹ️スケーリング（標準化・正規化）

異なるスケールの特徴量が混在すると、大きな値を持つ特徴量がモデルに過度な影響を与える可能性があります。

#### 標準化（Standardization）

データを平均0、標準偏差1の分布に変換します。
```
z = (x - μ) / σ
```

**適用条件**
- データが正規分布に従う場合
- 外れ値の影響を受けにくい場合
- 線形モデルやSVMに効果的

#### 正規化（Normalization）

データを0から1の範囲に変換します。
```
x_normalized = (x - min) / (max - min)
```

**適用条件**
- データの分布が均等な場合
- ニューラルネットワークに効果的
- 外れ値に敏感

#### ロバストスケーリング

中央値と四分位範囲を使用したスケーリングです。
```
x_robust = (x - median) / IQR
```

**適用条件**
- 外れ値が存在する場合
- 分布が歪んでいる場合

### ℹ️カテゴリ変数のエンコーディング

カテゴリ変数を数値形式に変換する手法です。統計学的性質を理解して適切な手法を選択することが重要です。

#### 順序エンコーディング（Ordinal Encoding）

順序関係のあるカテゴリ変数に適用します。
- 例：学歴（小学校=1、中学校=2、高校=3、大学=4）

#### ワンホットエンコーディング（One-Hot Encoding）

名義尺度のカテゴリ変数に適用します。
- カテゴリ数分のダミー変数を作成
- 多重共線性を避けるため、n-1個の変数を使用

#### ターゲットエンコーディング

目的変数の統計量でエンコーディングします。
- 回帰問題：カテゴリごとの目的変数の平均値
- 分類問題：カテゴリごとの陽性率

**注意点**
- オーバーフィッティングのリスク
- クロスバリデーションでの適切な実装が必要

## Pythonを用いた実践

diamondsデータセットを用いて、実際のデータ前処理を実装します。

### ライブラリのインポートとデータ読み込み

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# データの読み込み
df = sns.load_dataset('diamonds')
df_original = df.copy()  # 元データを保持

print("データ形状:", df.shape)
print("\n基本統計量:")
print(df.describe())
```

### 欠損値処理

```python
# 欠損値の確認
print("欠損値の状況:")
print(df.isnull().sum())

# 欠損値可視化
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 6))
df.isnull().sum().plot(kind='bar', ax=ax)
ax.set_title('欠損値の分布')
ax.set_ylabel('欠損値の数')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 実際には欠損値がないため、意図的に欠損値を作成（学習用）
# 実務では以下のような処理を行います
np.random.seed(42)
df_missing = df.copy()

# caratに一部欠損値を作成
missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
df_missing.loc[missing_indices, 'carat'] = np.nan

print(f"欠損値を作成後: {df_missing['carat'].isnull().sum()}個の欠損値")

# 欠損値処理の実装
# 方法1: 平均値補完
imputer_mean = SimpleImputer(strategy='mean')
df_missing['carat_mean_imputed'] = imputer_mean.fit_transform(df_missing[['carat']])

# 方法2: 中央値補完
imputer_median = SimpleImputer(strategy='median')
df_missing['carat_median_imputed'] = imputer_median.fit_transform(df_missing[['carat']])

print("補完前後の統計量比較:")
print("元データ:", df['carat'].describe())
print("平均値補完:", df_missing['carat_mean_imputed'].describe())
print("中央値補完:", df_missing['carat_median_imputed'].describe())
```

### 外れ値検出と処理

```python
# IQR法による外れ値検出
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# priceの外れ値検出
outliers, lower, upper = detect_outliers_iqr(df, 'price')
print(f"外れ値の数: {len(outliers)}")
print(f"外れ値の範囲: < {lower:.2f} または > {upper:.2f}")

# 外れ値の可視化
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# ボックスプロット
df.boxplot(column='price', ax=axes[0])
axes[0].set_title('Price Distribution (with outliers)')

# ヒストグラム
df['price'].hist(bins=50, ax=axes[1])
axes[1].set_title('Price Histogram')
axes[1].set_xlabel('Price')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# 外れ値処理：Winsorizing（上位5%を95%ile値に置換）
df_processed = df.copy()
upper_limit = df['price'].quantile(0.95)
df_processed['price_winsorized'] = df['price'].clip(upper=upper_limit)

print("Winsorizing前後の比較:")
print("元データ:", df['price'].describe())
print("処理後:", df_processed['price_winsorized'].describe())
```

### スケーリング

```python
# 数値変数の選択
numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z', 'price']
X_numeric = df[numeric_features].copy()

# 標準化
scaler_standard = StandardScaler()
X_standardized = pd.DataFrame(
    scaler_standard.fit_transform(X_numeric),
    columns=[f'{col}_standardized' for col in numeric_features]
)

# 正規化
scaler_minmax = MinMaxScaler()
X_normalized = pd.DataFrame(
    scaler_minmax.fit_transform(X_numeric),
    columns=[f'{col}_normalized' for col in numeric_features]
)

# スケーリング結果の比較
fig, axes = plt.subplots(3, 2, figsize=(12, 15))

# 元データ
X_numeric[['carat', 'price']].hist(ax=axes[0], bins=30)
axes[0, 0].set_title('Original - Carat')
axes[0, 1].set_title('Original - Price')

# 標準化データ
X_standardized[['carat_standardized', 'price_standardized']].hist(ax=axes[1], bins=30)
axes[1, 0].set_title('Standardized - Carat')
axes[1, 1].set_title('Standardized - Price')

# 正規化データ
X_normalized[['carat_normalized', 'price_normalized']].hist(ax=axes[2], bins=30)
axes[2, 0].set_title('Normalized - Carat')
axes[2, 1].set_title('Normalized - Price')

plt.tight_layout()
plt.show()

print("スケーリング後の統計量:")
print("\n標準化:")
print(X_standardized.describe())
print("\n正規化:")
print(X_normalized.describe())
```

### カテゴリ変数のエンコーディング

```python
# カテゴリ変数の処理
categorical_features = ['cut', 'color', 'clarity']

# 1. ラベルエンコーディング（順序がある場合）
# 品質グレードを数値に変換
cut_order = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}
color_order = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}
clarity_order = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}

df_encoded = df.copy()
df_encoded['cut_encoded'] = df_encoded['cut'].map(cut_order)
df_encoded['color_encoded'] = df_encoded['color'].map(color_order)
df_encoded['clarity_encoded'] = df_encoded['clarity'].map(clarity_order)

# 2. ワンホットエンコーディング
df_onehot = pd.get_dummies(df[categorical_features], prefix=categorical_features)

print("ワンホットエンコーディング結果:")
print(f"元のカテゴリ変数数: {len(categorical_features)}")
print(f"ワンホット後の変数数: {df_onehot.shape[1]}")
print("\n作成された変数:")
print(df_onehot.columns.tolist())

# エンコーディング結果の確認
print("\nラベルエンコーディング結果:")
for col in ['cut', 'color', 'clarity']:
    encoded_col = f'{col}_encoded'
    print(f"\n{col}:")
    comparison = df[[col, encoded_col]].drop_duplicates().sort_values(encoded_col)
    print(comparison)
```

### 統合的な前処理パイプライン

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# 前処理パイプラインの構築
def create_preprocessing_pipeline():
    # 数値変数の前処理
    numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # カテゴリ変数の前処理
    categorical_features = ['cut', 'color', 'clarity']
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
    ])
    
    # 前処理パイプラインの結合
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )
    
    return preprocessor

# パイプラインの適用
preprocessor = create_preprocessing_pipeline()

# 特徴量と目的変数の分離
X = df.drop('price', axis=1)
y = df['price']

# 前処理の実行
X_processed = preprocessor.fit_transform(X)

print("前処理後の形状:")
print(f"特徴量: {X_processed.shape}")
print(f"目的変数: {y.shape}")

# 特徴量名の取得
feature_names = (
    preprocessor.named_transformers_['num'].named_steps['scaler']
    .get_feature_names_out(['carat', 'depth', 'table', 'x', 'y', 'z']).tolist() +
    preprocessor.named_transformers_['cat'].named_steps['onehot']
    .get_feature_names_out(['cut', 'color', 'clarity']).tolist()
)

print(f"\n処理後の特徴量数: {len(feature_names)}")
print("特徴量名:")
for i, name in enumerate(feature_names):
    print(f"{i+1:2d}. {name}")
```

## まとめ

データ前処理は機械学習の成功を左右する重要なステップです。統計学的知識を活用することで：

1. **客観的な判断基準**：データの分布や特性に基づいた処理方法の選択
2. **適切な手法選択**：問題設定とデータ特性に応じた最適な前処理
3. **処理結果の評価**：統計的指標による処理効果の定量的評価

が可能になります。

次回は、「特徴量エンジニアリング」をテーマに、前処理されたデータから予測精度向上に寄与する新たな特徴量を創出する手法について詳しく解説します。
ぜひ引き続きご覧ください。